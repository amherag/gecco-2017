\section{Introduction}
\label{introduction}

%The particle swarm optimization (PSO) algorithm is an evolutionary algorithm (EA) distinguished by its suitability for optimization problems with continuous search spaces, in contrast to other EAs that are more suitable for combinatorial problems, such as genetic algorithms (GA). The reason behind this is that the population of solutions in the PSO algorithm (labeled as particles) move in the search space in a continuous manner, according to a direction dictated by the best particle or solution. As in the case of GA and other EA, PSO has a better possibility of converging to a solution if its population has high diversity (i.e. the particles are dispersed in the search space) \cite{morrison2001measurement}. In general, a bigger population in any EA should give solutions closer to the global minimum, but the algorithm will take more computational resources.

%If a problem is sufficiently complex to require a relatively large number of particles in order to achieve a solution with a high fitness, the first and obvious solution is to increase the computational power available to the algorithm. Another solution, and one that has increased in popularity in the last decade, is to implement a distributed architecture. This kind of solutions usually partition the search space into a number of search sub-spaces equivalent to the number of devices participating in the distributed architecture \cite{sahin2007distributed}. A consequence of this approach is that several different instances of an EA will be running, one in each of the devices, and each of these instances can be configured with different values for their parameters. This could be seen as a drawback, as now one has to think on what parameters are the optimal for each device, or implement optimization algorithms to determine such values.

%Another problem arises with such distributed architectures: how are the populations going to be managed. One could only instruct each device to perform iterations (or \textit{generations}, using the EA jargon) using randomly generated solutions (or \textit{individuals}) in its corresponding search subspace, and after \textit{N} iterations the best solution is sent to the server \cite{sahin2007distributed}. Another solution is to follow the island-based model. % Add a reference
%In this model, certain individuals are sent to the server,
% In the original island model, they are sent directly to other
% islands, this is the pool based approach.
%and the server sends them to other devices and, therefore, to other search spaces. In other words, it can be said that individuals are traveling from one device (an \textit{island}) to another. The purpose of this technique is to influence the solutions of a particular search sub-space with a solution that was evolved in a different search sub-space, in hopes of making the architecture converge faster and avoid local minima.

%A similar approach to the one taken in the island-based model is the pool-based % Here you are describing the EvoSpace model
%model. In the pool-based model, every individual across every generation is controlled by the server. Devices in the parallel or distributed architecture extract a sample of individuals from the pool in the server. This sample is often determined randomly, as well as the size, although the size could be established in terms of the device's computational capacity, set to a fixed number, or determined by other factors. After the sample is gathered, it is sent to the device to be subjected to the evolutionary process, which can last for an arbitrary number of generations. If the EA finished successfully, the evolved sample is returned to the server pool (and if it fails, one of multiple processes can be performed, for example, the sample can be re-spawned in the server after a fixed amount of time, or the sample could get deleted). The individuals in the returned samples from each device can then be part of other samples that will be sent to another device. As a result, the major difference between the island-based model and the pool-based model is that the server can only examine those individuals who are migrating to other islands in the former, and the server can examine any individual at certain points in time during the evolutionary process in the latter \cite{garcia2014randomized}.

%As mentioned before, a problem that arises when implementing an EA solution is determining the values of the different parameters of the algorithm. For example, in the case of the GA algorithm, one has to set the number of individuals in the initial population, what type of crossover methods are going to be used, the mutation and the crossover chance, to name some of the parameters. In order to determine the optimal values, several methods can be used, where choosing a value using one's experience would be the most simple of them. Another approach is to use another optimization algorithm, such as an EA, to tune the first EA, although this could result in an unsatisfying solution, because it adds another layer of parameters to be tuned. Instead of choosing a fixed set of values for the parameters of an EA, one can implement a controller that dynamically adapts the parameters' values according to some criteria. As an example, in the work by Melin et al. \cite{melin2013optimal},
% Justify why use this paper in your work
%a fuzzy inference system is used as a controller to be changing the social and cognitive factors in the PSO, based on the current iteration the algorithm is at, the diversity of the particles, and the error among the particles. As can be noted, each of these methods (with the exception of the arbitrary selection of values) add an extra layer of complexity to the initial problem.

This work proposes the use of a randomized parameter setting (RPS) strategy to determine the values of the parameters in the PSO instances in a pool-based model. The benefit of implementing this strategy is that the aforementioned layer of complexity does not need to be added; all the parameters are set to random values, and implementing a parallel or distributed architecture for the pool-based model is already justified by its benefits in the increase of computational resources available for the optimization process. The drawback for this approach could be that setting the parameters in a randomly fashion is not going to be as effective as using a dynamic adapter, use another optimization method, or even set the parameters manually. This work presents a series of experiments that support the claim that the proposed method is as effective as a dynamic adapter, specifically a fuzzy inference system. %The hypothetical explanation for such results is that the PSO instances with randomized parameter settings help increase the position diversity of the particles, as each particle will participate in the evolutionary process of multiple PSOs.

% Mention the hypothesis of the increase of position diversity and the random parameter.

% Cambiar en el abstract por algo más como en este párrafo, además de considerar las sugerencias del doctor

\section{Proposed Method}
\label{proposed-method}

The proposed method is designed to accelerate the convergence of the
PSO algorithm. The hypothesis that was taken into consideration for this method is
that extending PSO to a parallel architecture should increase the
position diversity of the particles, and avoid premature convergence around local optima and should yield better results because of this, as explained
by S. Cheng in \cite{cheng2013population}. Furthermore, each device in
the parallel architecture that is searching for a solution is configured with a
random set of parameters, as proposed by Y. Gong and A. Fukunaga in \cite{gong2011distributed}. This aproach, as \cite{gong2011distributed} expresses it, exploits the fact that with a sufficient number of instances of an EA, there is a high probability that one of the instances will have a set of randomized parameters that performs well on a given problem.

\section{Experiments}
\label{experiments}

In order to compare the proposed method presented in Section \ref{proposed-method}, a set of other parameter tuning strategies were defined. These strategies are based on the ones presented in the work by Melin et al. \cite{melin2013optimal}, where a dynamic adaptation of the cognitive and the social factors in the particle swarm optimization algorithm is performed using a fuzzy inference system.% The authors of the aforementioned work propose three different dynamic adaptation of parameters strategies: 1) a fuzzy inference system takes as inputs the current number of iterations and the diversity of the particles, and give as output the social and cognitive factors; 2) a fuzzy inference system takes as inputs the current number of iterations and the error among the particles, and give as output the social and cognitive factors; and 3) a fuzzy inference system takes as inputs the current number of iterations, the diversity of the particles, and the error among the particles, and give as output the social and cognitive factors.

The objective of the work in \cite{melin2013optimal} was to determine what combination of inputs was better to perform a dynamic adaptation of parameters in a PSO. In the case of the present work, the authors propose that a randomized parameterization will achieve better or similar results than any of the proposed strategies in \cite{melin2013optimal}, supporting the idea that a good parameter tuning in EAs can be achieved by randomized parameterization.

To give shorter descriptions of the dynamic adaptation strategies, it can be mentioned that all of the membership functions are described by triangular functions. For a triangular membership function, one has to define three points (\textit{a}, \textit{b}, and \textit{c}), where $f(a) = 0$, $f(b) = 1$, $f(c) = 0$, and $a <= b <= c$. The social and the cognitive factors, represented by $c1$ and $c2$, range in domain from 0 to 3, as values in this interval are recommended in the literature \cite{kenndy1995particle}. The rule bases for the strategies are obtained from \cite{melin2013optimal}.

The first strategy, called Fuzzy PSO 1, is configured as follows. The first antecedent, \textit{iterations}, has a domain from 0 to 1, which represents the percentage of iterations completed at a certain point, and is described by three adjectives: low, medium, and high. In the case of \textit{low}, $a = 0$, $b = 0$, and $c = 0.5$; for \textit{medium}, $a = 0$, $b = 0.5$, and $c = 1$; and for \textit{high}, $a = 0.5$, $b = 1$, and $c = 1$. The second antecedent, \textit{diversity}, has a domain from 0 to 1, and is described by three adjectives: low, medium, and high. In the case of \textit{low}, $a = 0$, $b = 0$, and $c = 0.5$; for \textit{medium}, $a = 0$, $b = 0.5$, and $c = 1$; and for \textit{high}, $a = 0.5$, $b = 1$, and $c = 1$. For the consequents, \textit{c1} and \textbf{c2}, both have a domain from 0 to 3, and are described by five adjectives each: low, medium-low, medium, medium-high and high. In the case of \textit{low}, $a = 0$, $b = 0.5$, and $c = 1$; for \textit{medium-low}, $a = 0.5$, $b = 1$, and $c = 1.5$; for \textit{medium}, $a = 1$, $b = 1.5$, and $c = 2$; for \textit{medium-high}, $a = 1.5$, $b = 2$, and $c = 2.5$; lastly, for \textit{high}, $a = 2$, $b = 2.5$, and $c = 3$.

The second strategy, called Fuzzy PSO 2, is similar to Fuzzy PSO 1, with a slight difference in the antecedents. Instead of \textit{iterations} and \textit{diversity}, it uses \textit{iterations} and \textit{error}. This antecedent, \textit{error}, has a domain from 0 to 1, and is described by three adjectives: low, medium, and high. In the case of \textit{low}, $a = 0$, $b = 0$, and $c = 0.5$; for \textit{medium}, $a = 0$, $b = 0.5$, and $c = 1$; and for \textit{high}, $a = 0.5$, $b = 1$, and $c = 1$. The remaining antecedent and consequents are described the same way as in Fuzzy PSO 1.

For the third strategy, called Fuzzy PSO 3, \textit{iterations}, \textit{diversity} and \textit{error} are considered as antecedents, and \textit{c1} and \textit{c2} as the consequents. All of them are defined the same as in Fuzzy PSO 1 and Fuzzy PSO 2.



In the case of the proposed method, five instances of PSO are created, which are going to be drawing individuals from the population in the server pool. Every time an instance obtains a new sample of individuals, it performs an evolutionary process of only one generation, using randomized values for \textit{c1} and \textit{c2}. The sample size will be $N/5$, meaning that each device will have an equally sized sample. The samples always gather the individuals by random choice.

For each of the strategies mentioned in this Section, an initial population of 200 individuals is randomly generated, and they will be evolved for 100 iterations.% The objective for these strategies is to obtain the global minimum for the following benchmark functions: Sphere, Ackley, Rastrigin, Griewank, and Schaffer (number 2).

%While the strategies run, the diversity in position of the particles is recorded in every iteration. In addition to diversity,
The number of evaluations required for each method to reach certain threshold of error is recorded. In order to avoid converging to said threshold too soon or too late, a simple PSO with $c1 = 1$ and $c2 = 3$ is run 100 times, and the error obtained by the 90th percentile is used as the threshold. The reason behind this is that if a very low threshold is used, the strategies could never converge to it, and every strategy would report similar number of evaluations. Likewise, if a very high threshold is used, the strategies could converge to it too early, and every strategy would also report similar number of evaluations.

% Rule base for three methods
% Membership functions for three methods

\section{Results}
\label{results}

The basic procedure is to perform comparisons between each strategy against the proposed method, called PB-RPS PSO (pool-based randomized parameter settings PSO) in the results table. These comparisons take into consideration the number of evaluations required to achieve certain threshold, as is explained in Section \ref{experiments}.% For these comparisons, five benchmark functions are used: Sphere, Ackley, Rastrigin, Griewank, and Schaffer (number 2).

For the comparisons, 100 runs of each strategy are performed, and the number of evaluations is recorded after each run. The average and standard deviations are calculated and used to perform hypothesis tests. In the case of the comparisons in terms of diversity, 30 runs of each strategy are performed, for 100 generations. After each generation finishes its evolutionary process, the diversity in the particles is recorded, giving as result a record of 100 diversities for each of the 30 runs. In order to obtain the final averages and standard deviations for each strategy, the average and standard deviation of the 100 generations is calculated for each of the 30 runs, and the average is calculated for these 30 averages and 30 standard deviations, and they are used to perform hypothesis tests.

For the diversity comparisons, the hypothesis tests are considering that the proposed method obtains a higher diversity in its population. In the case of the number of evaluations, what is being considered is that the proposed method obtains a lower number than the other strategies. The results tables %

Table \ref{evaluations-rastrigin} shows the results for the Rastrigin benchmark function. This Table shows t-Values and confidence intervals to illustrate how well the proposed method performed against the corresponding strategy. If a hyphen is printed instead of a confidence interval, this means that the proposed method performed similarly than the corresponding strategy (a t-Value corresponding to a confidence interval of less than 80\% was calculated).
% The proposed method performed similarly in the cases of Fuzzy PSO 1 and Fuzzy PSO 2, but outperformed Fuzzy PSO 1 with a confidence interval of 90\%. In the case of diversity comparisons, the proposed method outperformed the other strategies.

\begin{table}[!t]
  \renewcommand{\arraystretch}{1.3}
  \caption{Comparison of Number of Evaluations for Each Method Using the Rastrigin Function Benchmark}
  \label{evaluations-rastrigin}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Method & $\mu$ & $SD$ & $n$ & t-Value & CI \\
    \hline
    PSO 		& 4585.93 & 6756.02 & 100 & 3.3529 & $>$ 99.8\% \\
    \hline
    Fuzzy PSO 1 & 3148.97 & 5625.51 & 100 & 1.6683 & $>$ 90\% \\
    \hline
    Fuzzy PSO 2 & 2332.24 & 4056.50 & 100 & 0.5252 & - \\
    \hline
    Fuzzy PSO 3 & 2648.31 & 4808.98 & 100 & 1.0102 & - \\
    \hline
    PB-RPS PSO  & 2055.44 & 3363.98 & 100 &  &  \\
    \hline
  \end{tabular}
\end{table}
% Critical t 99.8% 3.140569, Critical t 99.9% 3.350726
% Critical t 90.0% 1.654326


\section{Conclusions}
\label{conclusions}

The results obtained in this work support the use of randomized parameterization in the particle swarm optimization algorithm when used in a pool-based model. % The use of a strategy for parameter tuning should not be necessary for several optimization problems if using the strategy described in the proposed method.
%
A high diversity in the particles should help the algorithm to not fall into local minima prematurely, and a low number of evaluations means less computational power is needed in order to obtain satisfactory results. %Furthermore, the high diversity could be the reason for the high convergence rate of the proposed method, but this hypothesis would need a different set of experiments to be proved.
